{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bayesian Statistic and Naive Bayes [NB] classifiers</h1>\n",
    "\n",
    "<p>Naive Bayes (NB) is a machine learning model with high interpretability, we can use it to explore features importance and feature interaction. In this notebook we will:</p>\n",
    "<ul>\n",
    "    <li> What is NB and how is it applied in Data Science domain?</li>\n",
    "    <li> Is it suited to petfinder.my task?</li>\n",
    "    <li> How can we apply it to petminder.my dataset in this competition?</li>\n",
    "    <li> What insight on the features can we draw from it?</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.  What is NB and how is it applied in Data Science domain?</h1>\n",
    "\n",
    "<a  href=\"https://www.youtube.com/watch?v=z5UQyCESW64&t=625s\">Excellent 11 mins video from Andrew Ng, explaining Naive Bayes </a>\n",
    "<a href=\"https://machinelearningmastery.com/better-naive-bayes/\">12 tips on how to implement NB</a>\n",
    "\n",
    "<p>Main Idea: instead of learning the Adoption Speed distribution, we try to learn the feature distribution (i.e. how is Breed1 distributed? ) and then use Bayes Inference to infer Adoption Speed distribution</p>\n",
    "\n",
    "<p>The most important part of NB is that it assumes that features are indipendent, that's why is called Naive</p>\n",
    "\n",
    "<p>Maily used for spam filters, work great when features are indipendent.</p>\n",
    "\n",
    "<p>Here are some other competitions where NB was used succefuly</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Large Scale Hirearchical Text Classification:</h3> <a href=\"https://www.kaggle.com/c/lshtc/discussion/7980\">winning solution discussion</a> and <a href=\"http://blog.kaggle.com/2014/05/15/winners-in-large-scale-hierarchical-text-classification-team-anttip/\">kaggle blog</a>\n",
    "\n",
    "<p>So this is text-classification, \"classify Wikipedia documents into one of 325,056 categories\".</p>\n",
    "<p>Quoting from the winning solution \"[..]Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an <b>ensemble of sparse generative models extending Multinomial Naive Bayes</b>. The base-classifiers consist of <b>hierarchically smoothed models (?)</b>combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores.[..]\"</p>\n",
    "<br>\n",
    "<p><b>COMMENT</b>: I don't really get a lot from this, beside that he is EXTENDING NB, what does this mean? the competition seems to different from our (text based and pretty old, uses different technologies I never heard of), I will come back later</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Facebook V: predicting checkins</h3> <a href=\"http://blog.kaggle.com/2016/08/18/facebook-v-predicting-check-ins-winners-interview-3rd-place-ryuji-sakata/\">3 place Kaggle blog</a> and <a href=\"https://www.kaggle.com/c/facebook-v-predicting-check-ins/discussion/22078#126235\">discussion</a>\n",
    "\n",
    "<p>this is pretty unique, classification with HUGE number of classes: The goal of this competition is to predict which place a person would like to check in to. For the purposes of this competition, Facebook created an artificial world consisting of more than <b>100,000 places (classes)</b>located in a 10 km by 10 km square. For a given set of coordinates, your task is to return a ranked list of the most likely places. </p>\n",
    "\n",
    "<p>FROM <a href=\"https://www.kaggle.com/c/facebook-v-predicting-check-ins/discussion/22112\">DISCUSSION</a>: Naive Bayes is a first assumption of the data distribution. When using the sklearn.naivebayes.GaussianNB, it also assumes the distribution of each feature in cell is unimodal. But doing a simple visualization of the x, y for given placeid will tell you that assumption is not very good. One way to improve the Naive Bayes is to use kernel density estimation (KDE) rather than one single gaussian to get the probability density function. I recommend this blog (http://www.mglerner.com/blog/?p=28) for nice explanation of KDE. <a href=\"https://github.com/aikinogard/5th_place_solution_facebook_check_ins/tree/master/single_model/nb\">IMPLEMENTATION IN PYTHON ON GITHUB HERE</a></p>\n",
    "\n",
    "<p><b>COMMENT:</b> also this competition is pretty different from current one, not many people used NB but there is a super interesting insight, that if distribution is not Gaussian you can use this KDE,should read more about that</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Observing Dark Worlds:</h3> <a href=\"http://blog.kaggle.com/2012/12/19/a-bayesian-approach-to-observing-dark-worlds/\">2 place Kaggle blog</a>\n",
    "<p>This is complicated, you are given points in a 2-d plane and you need to classify \"is there  a HALO?\" where HALO is an attractor that make points form elliptic shapes</p>\n",
    "<p><b>COMMENT</b>: after looking into this, they use Bayesian methods but not NB. The winning solution is actually using <a href=\"https://stats.stackexchange.com/questions/165/how-would-you-explain-markov-chain-monte-carlo-mcmc-to-a-layperson\">Markov Chain Monte Carlo</a> somehow related to making random draws from target distribution</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Santander Customer Transactions</h3>: <a href=\"https://www.kaggle.com/blackblitz/gaussian-naive-bayes\">111 upvotes kernel</a>\n",
    "<p>This competition is more common: You are provided with an anonymized dataset containing numeric feature variables (gaussians), the binary target column, and a string ID_code column.</p>\n",
    "<p>The Gaussian naive Bayes classifier performs quite well on Santander Customer Trasaction data. This is because the <b>normality and independence assumptions are closely followed by the data</b>. We have seen that even if the data have been generated by independent normal distributions (according to the model trained on transformed data), we cannot get a better AUC. However, there may still be some other transformation that can improve our model. In my opinion, the normality assumption is not very realistic since some features seem to have lower and upper bounds.</p>\n",
    "<p>COMMENT: great kernel, updated with current technologies. A lot to learn from. The data tough with our kernel is pretty different, even if the problem is similar! <b>How is their anonymized data generated?</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. BNP Paribas Cardif Claims Management </h3><a href=\"https://www.kaggle.com/scirpus/benouilli-naive-bayes\">python script</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So this competition similar to the previous, anomyzed data (Gaussian + Multinomial) to predict a binary target.</p>\n",
    "<p>FROM SCRIPT COMMENTS: So, you did a leaky stack: Using NB predictions (Not out of fold) trained on one hot encoded categorical data as a (leaky but weak) meta feature to be used in XGB? Correct?</p>\n",
    "<p>FROM #2 winner <a href=\"https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/discussion/20252\">discussion</a>:  we found out <b>likelihood encoding</b> of interactions improving the result exponentially in the early stage of the competition. Since then, I mostly focused on creating different representation of the interactions in different levels </p>\n",
    "\n",
    "<p><b>COMMENTS</b>: no-one used NB, beside this script that got a super high score. (looks like ensemble of NB). Beside NB actually is super interesting to read how they deal with interaction between categorical variables. In one of the comment of #2 there are actually some interesting links to check-out -> like this https://www.kaggle.com/c/caterpillar-tube-pricing/discussion/15748, and also readmore about likelihood encoding, looks like that's why NB was working pretty good</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Two Sigma Market Predictions</h3>\n",
    "<a href=\"https://www.kaggle.com/ytokmakov/tell-her-about-bayes-and-350000-return\">NB kernel</a>\n",
    "<p><b>COMMENTS:</b> So this competition (that I did) was not at all about Bayesian methods. Interstingly enough  this guys uses NB and get a solid 0.52 (That was what you got with baseline XGBoost I think). He uses both Gaussian and Multinomial. No huge insights on NB but is a good working example on how to do it properly, and also a dimostration that actually works pretty good</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side Node: somehow related topic, <a href=\"https://www.kaggle.com/residentmario/linear-discriminant-analysis-with-pokemon-stats\">Linear Discriminant Analysis </a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Is it suited to petfinder.my task?</h1>\n",
    "<p>I am not really sure, I am doing it because I want to learn about it.</p>\n",
    "<p>To check if it suited for our dataset we mainly need to <b>check if we have indipendent features</b> or not to apply NB. In order to do this I did a notebook to explore correlations between our features <i>/exploratory-data-analysis/correlation_exploration.ipynb</i></p>\n",
    "\n",
    "<p>Here is the main observations on feature correlation from the notebook:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Correlation between categorical features</h2>\n",
    "<p>So turns out is not trivial to do this (there is not a pandas function), you need to do a Chi Squared test. I read about it, I understood something and I built this correlation heatmap</p>\n",
    "<img src=\"imgs/categorical_corr.png\">\n",
    "<p>Is broken (meaningless), because turns out that the \"degree of freedom\" is too high (not too sure). I gave up on finding correlation between categoricals. [could explore this better, doing correlation between two variables and so on, or transform to numerical]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Correlation between numerical features</h2>\n",
    "<p>This was easier, and a nice correlation plot came up. </p>\n",
    "<img src=\"imgs/numerical_corr.png\">\n",
    "<p>Some observation:\n",
    "<ul><li> the higher the age, the lower the quantity of pets (make sense)</li>\n",
    "<li> the higher the age, the higher the Fur Size (make sense)</li>\n",
    "<li> the higher the fur size, the higher the Fee (? interesting)</li></p>\n",
    "<p>Looks like we can use NB</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "<h1>3. How can we use it in this competition?</h1>\n",
    "<p>So is a bit tricky, because our features have different distributions (numerical feats have Gaussian distr, categorical feats has multinomial dist). I read a lot about how to do this, and basically this is how to use NB in this competition.\n",
    "<ul>\n",
    "    <li>train 1 NB on the numerical features</li>\n",
    "    <li>train 1 NB for every categorical (for technical reasons)</li>\n",
    "    <li>multiply the probabilities and that is you \"ensembled\" NB, you can multiply since you assume they are independent</li>\n",
    "    </ul>\n",
    "    I wrote more about this in the comments (and what SOF questions I read from) of the PredictiveModel class</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "<h1>4. What insight on the features can we draw from it?</h1>\n",
    "<p>So I implemented NB, on both categorical and numerical features. It is inside <i>models/NAIVE_BAYES/gaussianNaiveBayes.py</i> and <i>models/NAIVE_BAYES/multinomialNaiveBayes.py</i></p>\n",
    "<p>After implementing (and testing the model) I run a validation session to test the first results. I run on all categorical features one by one; this mean that I am training the model using only one feature at a time. Afterward I validate the model and this tells me how that feature is relevant. The results are interesting</p>\n",
    "<img src=\"imgs/NB_features.png\">\n",
    "<p>On the y axis is the score. I also trained the whole Gaussian variables on one NB and it got around 0.6.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
