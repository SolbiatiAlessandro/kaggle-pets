{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>features exploration with Naive Bayes classifier</h1>\n",
    "\n",
    "<p>Naive Bayes (NB) is a machine learning model with high interpretability, we can use it to explore features importance and feature interaction. In this notebook we will:</p>\n",
    "<ul>\n",
    "    <li> What is NB and how is it applied in Data Science domain?</li>\n",
    "    <li> Is it suited to petfinder.my task?</li>\n",
    "    <li> How can we apply it to petminder.my dataset in this competition?</li>\n",
    "    <li> What insight on the features can we draw from it?</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1.  What is NB and how is it applied in Data Science domain?</h1>\n",
    "\n",
    "<a  href=\"https://www.youtube.com/watch?v=z5UQyCESW64&t=625s\">Excellent 11 mins video from Andrew Ng, explaining Naive Bayes </a>\n",
    "\n",
    "<p>Main Idea: instead of learning the Adoption Speed distribution, we try to learn the feature distribution (i.e. how is Breed1 distributed? ) and then use Bayes Inference to infer Adoption Speed distribution</p>\n",
    "\n",
    "<p>The most important part of NB is that it assumes that features are indipendent, that's why is called Naive</p>\n",
    "\n",
    "<p>Maily used for spam filters, work great when features are indipendent. <b>[Read more on why and how is used for spam filters and why works good there]</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Is it suited to petfinder.my task?</h1>\n",
    "<p>I am not really sure, I am doing it because I want to learn about it.</p>\n",
    "<p>To check if it suited for our dataset we mainly need to <b>check if we have indipendent features</b> or not to apply NB. In order to do this I did a notebook to explore correlations between our features <i>/exploratory-data-analysis/correlation_exploration.ipynb</i></p>\n",
    "\n",
    "<p>Here is the main observations on feature correlation from the notebook:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Correlation between categorical features</h2>\n",
    "<p>So turns out is not trivial to do this (there is not a pandas function), you need to do a Chi Squared test. I read about it, I understood something and I built this correlation heatmap</p>\n",
    "<img src=\"imgs/categorical_corr.png\">\n",
    "<p>Is broken (meaningless), because turns out that the \"degree of freedom\" is too high (not too sure). I gave up on finding correlation between categoricals. [could explore this better, doing correlation between two variables and so on, or transform to numerical]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Correlation between numerical features</h2>\n",
    "<p>This was easier, and a nice correlation plot came up. </p>\n",
    "<img src=\"imgs/numerical_corr.png\">\n",
    "<p>Some observation:\n",
    "<ul><li> the higher the age, the lower the quantity of pets (make sense)</li>\n",
    "<li> the higher the age, the higher the Fur Size (make sense)</li>\n",
    "<li> the higher the fur size, the higher the Fee (? interesting)</li></p>\n",
    "<p>Looks like we can use NB</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "<h1>3. How can we use it in this competition?</h1>\n",
    "<p>So is a bit tricky, because our features have different distributions (numerical feats have Gaussian distr, categorical feats has multinomial dist). I read a lot about how to do this, and basically this is how to use NB in this competition.\n",
    "<ul>\n",
    "    <li>train 1 NB on the numerical features</li>\n",
    "    <li>train 1 NB for every categorical (for technical reasons)</li>\n",
    "    <li>multiply the probabilities and that is you \"ensembled\" NB, you can multiply since you assume they are independent</li>\n",
    "    </ul>\n",
    "    I Wrote more about this in the comments of the PredictiveModel class of naiveBayes</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "<h1>4. What insight on the features can we draw from it?</h1>\n",
    "<p>So I implemented NB, on both categorical and numerical features. It is inside <i>models/NAIVE_BAYES/gaussianNaiveBayes.py</i> and <i>models/NAIVE_BAYES/multinomialNaiveBayes.py</i></p>\n",
    "<p>After implementing (and testing the model) I run a validation session to test the first results. I run on all categorical features one by one; this mean that I am training the model using only one feature at a time. Afterward I validate the model and this tells me how that feature is relevant. The results are interesting</p>\n",
    "<img src=\"imgs/NB_features.png\">\n",
    "<p>On the y axis is the score. I also trained the whole Gaussian variables on one NB and it got around 0.6.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
